---
layout: post
title: "[KCC23 Workshop] Machine Comprehension Methodology for Legal Document Review Using a Generative Large Language Model"
collection: papers
permalink: /papers/main-legal-document-review/
date: 2024-06-25
authors: "HaeIn Jung, MinJu Kim, HeuiYeen Yeen, YoungSun Lim, Myoung-Wan Koo"
conference: "KCC 2023"
paper_link: "https://drive.google.com/file/d/1LQXITooG-Q5c9JAbvYUdsEWgdF5K16GJ/view?usp=sharing"
---

This paper is accepted at the Proceedings of the Korea Computer Congress (KCC) Explainable Artificial Workshop 2023.

## Abstract

This paper presents a methodology for machine comprehension in the Korean legal domain using a generative large language model. The study explores various prompting methodologies to enable the model to read articles of incorporation and provide appropriate answers to given questions. The performance of models such as GPT-3.5 and GPT-4 in the Korean legal domain is evaluated for the first time in this study. The analysis reveals that while simple prompting techniques enhance performance for standardized answer types, there are limitations for non-standardized types, highlighting the need for more sophisticated prompting methodologies in future research.

## Method

### Overview

This research leverages a generative large language model to address the task of reviewing articles of incorporation in the legal domain. The process involves extracting a checklist of questions that a lawyer must review from a given document and generating answers using the large language model.

### Data Collection

The articles of incorporation were collected through web crawling from the electronic disclosure system, gathering documents from 2000 listed companies. A team of lawyers then created a dataset by annotating questions and answers based on key areas such as stocks, shareholder meetings, and board of directors.

### Experiment Setup

1. **Prompt Design**: Different prompt designs were experimented with, including English and Korean explanations and the use of delimiters to help the model distinguish between the document and the questions.
2. **Model Evaluation**: The performance of GPT-3.5 and GPT-4 models was assessed using accuracy metrics. For objective and subjective questions, the answers generated by the models were compared to the annotated dataset.

## Results

- **True/False and Multiple Choice**: The models performed best when English explanations were included in the prompts. GPT-4 outperformed GPT-3.5 in these categories.
- **Subjective Questions**: There was no significant performance difference between using English and Korean explanations. Both GPT-3.5 and GPT-4 showed similar performance levels.
- **Comparison with Fine-Tuned Models**: Smaller fine-tuned models like T5-base showed better performance for objective questions, but generative models excelled in subjective question answering.

### Performance Summary

- **True/False Prompts**:
  - GPT-3.5: Up to 95.6% accuracy
  - GPT-4: Up to 95.6% accuracy
- **Multiple Choice Prompts**:
  - GPT-3.5: Up to 75.8% accuracy
  - GPT-4: Up to 75.8% accuracy
- **Subjective Prompts**:
  - GPT-3.5: Up to 70.2% accuracy
  - GPT-4: Up to 69.5% accuracy

## Value of the Paper

This study confirms the effectiveness of using large language models with prompt engineering for specialized tasks in the legal domain, specifically for the review of articles of incorporation. By comparing the performance of GPT-3.5 and GPT-4, the research provides insights into the strengths and limitations of current generative models. The findings suggest that while simple prompts are sufficient for standardized questions, more complex prompting techniques are necessary for non-standardized questions. This work sets the foundation for future research to develop advanced prompting methods that can further enhance the capabilities of large language models in legal and other specialized domains.

---
## Paper
<iframe src="https://drive.google.com/file/d/1LQXITooG-Q5c9JAbvYUdsEWgdF5K16GJ/preview" width="640" height="480"></iframe>
