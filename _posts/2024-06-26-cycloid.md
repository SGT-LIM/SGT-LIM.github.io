---
title: "[KCC23] Optimization Convergence Speed and Stability Improvement through Cycloid Approximation of MSE Loss Function"
collection: papers
permalink: /papers/main-cycloid-approximation/
date: 2024-06-26
authors: "Yunsu Lee, Heebin Yoo, Hyundo Lee, Youngsun Lim, Byoung-Tak Zhang"
conference: "KCC 2023"
paper_link: "https://drive.google.com/file/d/1dJIrp1Dlt_bvqMHeJH50T-Vlna7XhW_9/view?usp=sharing"
---

This paper is accepted at the Korea Computer Congress (KCC) 2023. This paper received the Best Presentation Award.

## Abstract

Cycloid is a curve traced by a point on a circle rolling along a straight line. It has two notable physical properties: it is the Brachistochrone curve, where an object falls the fastest under gravity, and the isochrone curve, where the time to reach the end point is the same regardless of the starting point. This study applies these properties to the loss function in machine learning. The backpropagation process, where the model updates weights to minimize loss, is akin to rolling an object downhill. We ...

## Method

### Overview

This study investigates the application of cycloid properties to the Mean Square Error (MSE) loss function in machine learning to improve convergence speed and stability. The momentum optimizer, which simulates inertia, is hypothesized to act similarly to gravity in this context.

### Cycloid Properties

- **Brachistochrone Curve**: Cycloid is the path of fastest descent under gravity. The gravitational acceleration on a cycloid is greater than on a straight line, leading to faster movement.
- **Isochrone Curve**: Regardless of the starting point, the time to reach the end of the cycloid curve is the same.

### Momentum Optimizer and Gravity

The momentum optimizer, which includes previous gradient information, is compared to a gravity-like force. The optimizer updates weights using past gradients, maintaining inertia similar to gravitational acceleration.

### Experimental Setup

1. **Model and Training**: A neural network with a momentum optimizer is trained on a regression problem for 1000 epochs.
2. **Loss Function Analysis**: The gradients of loss functions are compared to the cycloid curve to identify similarities.
3. **Performance Metrics**: The convergence speed and variance of loss functions similar to the cycloid are compared to the overall average.

## Results

- **Convergence Speed**: Loss functions resembling cycloids converged faster, with an average of 80.1 epochs compared to the overall average of 83.025 epochs.
- **Stability**: Cycloid-like loss functions showed lower variance (1.09) compared to the overall average (2.33).

### Performance Summary

- **Convergence Epoch**:
  - Cycloid-like MSE: 80.1 epochs
  - Average MSE: 83.025 epochs
- **Variance**:
  - Cycloid-like MSE: 1.09
  - Average MSE: 2.33

## Value of the Paper

This research demonstrates that applying the physical properties of the cycloid curve to the MSE loss function can significantly enhance the convergence speed and stability of machine learning models. The findings suggest that loss functions designed with cycloid characteristics may offer superior performance, providing a novel approach to optimizing machine learning algorithms. Future research will explore more complex and general environments to further validate these results.

---

## Paper
<iframe src="https://drive.google.com/file/d/1dJIrp1Dlt_bvqMHeJH50T-Vlna7XhW_9/preview" width="640" height="480"></iframe>
