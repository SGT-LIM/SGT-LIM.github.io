---
title: "[WACV '25] Label-Augmented Dataset Distillation"
collection: papers
permalink: /papers/main-label-augmented-dataset-distillation/
date: 2024-07-20
conference: "Winter Conference on Applications of Computer Vision (WACV) 2025"
paper_link: "https://arxiv.org/abs/2409.16239"
---

This paper, authored by Seoungyoon Kang\*, **Youngsun Lim\***, and Hyunjung Shim (*Equal Contribution), has been accepted for presentation at the **Winter Conference on Applications of Computer Vision (WACV) 2025**. I’m deeply grateful for this opportunity and proud that our work is among the 167 papers accepted out of 1381 submissions in Round 1.

## Abstract
Traditional dataset distillation primarily focuses on image representation while often overlooking the important role of labels. In this study, we introduce Label-Augmented Dataset Distillation (LADD), a new dataset distillation framework enhancing dataset distillation with label augmentations. LADD sub-samples each synthetic image, generating additional dense labels to capture rich semantics. These dense labels require only a 2.5% increase in storage (ImageNet subsets) with significant performance benefits, providing strong learning signals. Our label generation strategy can complement existing dataset distillation methods for significantly enhancing their training efficiency and performance. Experimental results demonstrate that LADD outperforms existing methods in terms of computational overhead and accuracy. With three high-performance dataset distillation algorithms, LADD achieves remarkable gains by an average of 14.9% in accuracy. Furthermore, the effectiveness of our method is proven across various datasets, distillation hyperparameters, and algorithms. Finally, our method improves the cross-architecture robustness of the distilled dataset, which is important in the application scenario.

## Method

We propose Label-Augmented Dataset Distillation (LADD), a specialized label augmentation method for dataset distillation. During the dataset distillation stage, LADD conducts a label augmentation process to images distilled by conventional image-level dataset distillation algorithms. For each image x, we produce additional groups of soft labels, denoted dense labels, and create a label-augmented dataset DLA. Specifically, to obtain DLA, the label augmentation step goes through two processes: (1) an image sub-division and (2) a dense label generation. In the deployment stage, LADD uses both global (i.e., full images with hard labels) and local data (i.e., sub-sampled images with dense labels) to effectively train the network.

**Image Sub-Sampling:** We define a function S that samples synthetic image xi ∈ D into several sub-images. Each sub-image covers R% of each axis. To achieve a uniform sampling across xi, we maintain a consistent stride (100%−R%)/(N − 1) for cropping. After the sub-sampling, we resize each sub-image to match the dimension of xi.

**Dense Label Generation:** We generate labels for each sub-image xi,j , resulting in N2 labels for each synthetic image xi. We develop the labeler ys = g(x), where x denotes the image and ys is the corresponding soft label. We train the labeler on the source dataset Ds from scratch. Then, we obtain a dense label yd from each sub-image. The final label augmented dataset is denoted as DLA = {(xi, yi, yd i )|i ∈ [1, C × IPC]}.

## Results

Experimental results validate the key advantages of our LADD. At 5 IPC, LADD consistently surpasses the 6 IPC baseline while consuming 87% less memory. Additionally, in this setup, LADD only requires an extra 0.002 PFLOPs for label augmentation compared to the 5 IPC baseline. This is notably lower than the additional 211 PFLOPs required by the 6 IPC setup. Furthermore, LADD improves the performances of three baselines by an average of 14.9%, validated across five test model architectures and five distinct datasets. Finally, GradCAM visualizations show that LADD-trained models capture objects within images more accurately, demonstrating the robustness of our label-augmented distilled dataset approach.

## Value of the Paper

This paper highlights the overlooked role of labels in dataset distillation and introduces a novel framework, Label-Augmented Dataset Distillation (LADD). The contributions include:

- Recognition of the crucial role of labels in dataset distillation.
- Introduction of a novel framework that utilizes dense labels for local views of each synthetic image.
- Extensive experiments revealing significant improvements in computation efficiency, storage efficiency, and cross-architecture robustness.
- Demonstration of the method's effectiveness across various datasets, distillation hyperparameters, and algorithms.

By addressing the limitations of current methods and leveraging the potential of label augmentations, this paper significantly advances the field of dataset distillation.

## Paper
You can access the full paper [here](https://arxiv.org/pdf/2409.16239) or preview it below:

<iframe src="https://arxiv.org/pdf/2409.16239" width="640" height="480"></iframe>
