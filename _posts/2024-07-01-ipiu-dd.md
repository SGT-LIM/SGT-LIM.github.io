---
title: "[IPIU24] Efficient Dataset Distillation for Storage Space Based on Class Characteristics"
collection: papers
permalink: /papers/main-dataset-distillation/
date: 2024-07-01
authors: "Seungyoon Kang, Yoonji Jung, Youngsun Lim, Seohyun Lim, Hyunjung Shim"
conference: "IPIU 2024"
paper_link: "https://drive.google.com/file/d/1kRmpn6Y8eh3WdAslFfS_54w_qnrsUbLC/view?usp=sharing"
---

This paper is accepted at the 36th Workshop on Image Processing and Image Understanding (IPIU).

## Abstract

Dataset distillation aims to create a new dataset that is smaller and easier to manage while maintaining the core information and characteristics of the original large dataset. The goal is to achieve performance comparable to using the original dataset with a smaller distilled dataset. This study introduces a new approach focused on the number of images per class, assuming that different classes require different numbers of images to represent their essential features. We proposed a model that adjusts th...

## Method

### Overview

Dataset distillation is critical in areas such as continuous learning, neural architecture search, and privacy preservation. This research focuses on the number of images per class and proposes a model that adjusts the number of images per class to efficiently represent the dataset.

### Dataset Distillation Methodologies

1. **Performance Matching**: Adjusting the performance of models trained on the distilled dataset to match those trained on the original dataset.
2. **Distribution Matching**: Ensuring the statistical properties of the distilled dataset are similar to those of the original dataset.
3. **Parameter Matching**: Matching the parameters of the student model to those of the teacher model across multiple steps.

### Experimental Setup

1. **Class-specific Image Count**: Using embedding visualization and K-means clustering to determine the optimal number of images per class.
2. **Model Training**: Training a convolutional neural network on the distilled dataset and comparing its performance to the baseline.

## Results

- **Class-specific Image Count**: Classes with fewer image patterns required fewer images to represent them, while classes with more diverse patterns required more images.
- **Performance**: The proposed model achieved competitive performance on the ImageNette dataset, with accuracy surpassing the baseline model by approximately 0.3%.

### Performance Summary

- **Accuracy on ImageNette**:
  - Full Dataset: 87.4%
  - Baseline Distilled Dataset: 66.6%
  - Proposed Model: 66.9%

## Value of the Paper

This study presents a novel approach to dataset distillation by focusing on the characteristics of each class and adjusting the number of images per class accordingly. The findings highlight the importance of considering class-specific characteristics for efficient storage space management and demonstrate that the proposed method achieves significant performance improvements over baseline methods.

---

## Paper
<iframe src="https://drive.google.com/file/d/1kRmpn6Y8eh3WdAslFfS_54w_qnrsUbLC/preview" width="640" height="480"></iframe>
